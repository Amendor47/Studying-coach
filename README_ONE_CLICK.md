# üöÄ Coach de R√©vision - Lancement en 1 Clic

## üéØ D√©marrage Ultra-Rapide

### macOS / Linux
```bash
./start-coach.command
```
ou
```bash
./start-coach.sh
```

### Windows
Double-cliquez sur `start-coach.bat` ou ex√©cutez dans PowerShell/CMD:
```cmd
start-coach.bat
```

## ‚ú® Ce qui se passe automatiquement

1. **V√©rification syst√®me** - Python3, d√©pendances
2. **Installation automatique** - Environnement virtuel + packages
3. **Configuration Ollama** - D√©tection et t√©l√©chargement des mod√®les IA
4. **Port libre** - S√©lection automatique (5000-5010)
5. **Lancement** - Serveur + ouverture navigateur
6. **Mode d√©grad√©** - Fonctionne m√™me sans Ollama

## üõ†Ô∏è Diagnostic et D√©pannage

### Scripts de diagnostic
```bash
# Diagnostic complet du syst√®me
./tools/doctor.sh

# Tests de fum√©e (endpoints)
./tools/smoke.sh
```

### Modes de fonctionnement

#### Mode Complet (Ollama + Mod√®les)
- ‚úÖ Analyse IA avanc√©e
- ‚úÖ Am√©lioration de texte par LLM
- ‚úÖ G√©n√©ration de fiches intelligente
- ‚úÖ Embeddings s√©mantiques

#### Mode D√©grad√© (Sans Ollama)
- ‚úÖ Analyse hors-ligne des documents
- ‚úÖ Extraction de concepts automatique  
- ‚úÖ G√©n√©ration de QCM/QA basique
- ‚ö†Ô∏è Am√©lioration IA simul√©e (mock)

## üìã Fonctionnalit√©s Disponibles

### üìö Import et Analyse
- **Drag & Drop** - PDF, DOCX, TXT, MD
- **Analyse automatique** - Structure, concepts, r√©sum√©
- **OCR** - Extraction depuis images (si Tesseract install√©)

### üéØ Syst√®me de R√©vision
- **Flashcards adaptatifs** - Algorithme de r√©p√©tition espac√©e
- **QCM intelligents** - Questions √† choix multiples
- **Exercices pratiques** - Textes √† trous, associations
- **Pomodoro int√©gr√©** - Sessions chronom√©tr√©es

### ü§ñ IA Optionnelle
- **Am√©lioration de contenu** - Via Ollama/LLM
- **G√©n√©ration de questions** - Contextuelle et pertinente
- **R√©sum√©s automatiques** - Extraction des points cl√©s

## üîß Configuration Avanc√©e

### Variables d'environnement (.env)
```bash
# Ollama
OLLAMA_HOST=http://127.0.0.1:11434
OLLAMA_MODEL=llama3:8b

# D√©veloppement
CORS_ORIGINS=http://127.0.0.1:5000,http://localhost:5000
FLASK_ENV=development
DEBUG=true
```

### Configuration LLM (settings-local.yaml)
```yaml
provider: ollama
model: llama3:8b
embedding_model: nomic-embed-text
timeout_s: 60
temperature: 0.1
```

## üêõ Debug et D√©veloppement

### Outils de debug int√©gr√©s
- **Mode debug CSS** - Touche `~` pour visualiser zones cliquables
- **Overlay d'erreur JS** - Affichage automatique des erreurs
- **Sonde DOM** - Inspection interactive des √©l√©ments
- **Monitor de performance** - Suivi des m√©triques temps r√©el

### Endpoints utiles
```
GET  /api/health          # Sant√© g√©n√©rale
GET  /api/health/llm      # Sant√© LLM/Ollama  
POST /api/upload          # Upload fichier
POST /api/improve         # Am√©lioration IA
POST /api/offline/analyze # Analyse hors-ligne
```

## üö® D√©pannage Courant

### "Python non trouv√©"
- **macOS/Linux**: `brew install python3` ou gestionnaire de paquets
- **Windows**: T√©l√©charger depuis [python.org](https://python.org/downloads/) et cocher "Add to PATH"

### "Ollama non accessible" 
- **Installation**: [ollama.ai/download](https://ollama.ai/download)
- **D√©marrage**: `ollama serve` dans un terminal s√©par√©
- **Mod√®les**: `ollama pull llama3:8b && ollama pull nomic-embed-text`

### "Port occup√©"
- Le script trouve automatiquement un port libre (5000-5010)
- Forcer un port: `PORT=5005 ./start-coach.sh`

### Interface ne s'affiche pas
- V√©rifier dans la console du navigateur (F12)
- Activer debug mode: touche `~` sur la page
- Consulter les logs: `logs/app.log`

## üìà Performance et Optimisation

### Recommandations syst√®me
- **RAM**: 4GB minimum, 8GB recommand√© pour Ollama
- **Stockage**: 10GB libres pour mod√®les Ollama
- **Navigateur**: Chrome/Firefox/Safari r√©cent

### Optimisations automatiques
- Cache intelligent des r√©ponses LLM
- Compression des assets statiques
- Chargement diff√©r√© des scripts
- Service Worker d√©sactiv√© en dev

## üîí S√©curit√© et Donn√©es

### Protection des donn√©es
- **Traitement local** - Aucune donn√©e envoy√©e √† des serveurs externes
- **Mod√®les locaux** - Ollama fonctionne enti√®rement hors-ligne
- **Cache s√©curis√©** - Donn√©es temporaires chiffr√©es
- **CORS limit√©** - Origines restreintes en production

---

## üí° Support et Contribution

- **Issues**: Utiliser le syst√®me de tickets GitHub
- **Debug**: Joindre la sortie de `./tools/doctor.sh`
- **Logs**: Fichier `logs/app.log` pour les erreurs d√©taill√©es
- **Tests**: Ex√©cuter `./tools/smoke.sh` pour valider le fonctionnement